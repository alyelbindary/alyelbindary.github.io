---
title: "From Attention to Education: T5utor Is Really All You Need"
collection: publications
permalink: /publication/2024-06-14-mnlp
excerpt: 'This paper corresponds to the project : "Create an AI ChatBot Specialized to course content at EPFL", which can be found within the "CV" section of this blog.'
date: 2024-06-14
venue: ''
slidesurl: 
paperurl: 'http://alyelbindary.github.io/files/MNLPredators.pdf'
siteurl:
citation: ''
---

Abstract
===

LMs have a wide range of practical applications in the Education sector where they can improve the development of personalized teaching as well as provide tutoring for student communities lacking standard educational resources. We have built an educational assistant using Flan-T5 large, a modestly sized LM able to run on consumer hardware which students can use to assist them in question-answering tasks. To further increase its capabilities we have incorporated RAG to leverage a large knowledge database as well as used 4-bit quantization to build a more memory-efficient model, which is crucial to effectively democratizing LLMs access. Model quantization leads to a 68% reduction in memory footprint without severely impacting performance remaining at 32% accuracy. Furthermore, our flagship model T5utor using RAG increased performance by 4 4% with respect to our MCQA model. After our training pipelines some of the models reach higher accuracies on common benchmarks w.r.t. the reference model.

<img src="../files/full_pipeline_new.jpg" alt="Full Model Training Pipeline" width="700" height="1400">